What is still missing for robust multi-node testing

This is the important part.

⚠️ 1. Your tests don’t yet assert invariants

Right now, most cluster tests answer:

“Does it seem to work?”

Robust testing answers:

“What must never happen, and do we detect it?”

You need explicit invariants, enforced either in:

tests, or

runtime checks (preferred)

Critical invariants you should assert

At most one primary per shard per term

Writes only accepted by ACTIVE primary

Term never goes backwards

WAL replay never applies foreign shards

MIGRATING_IN shard never accepts writes

Right now these are assumed, not asserted.

⚠️ 2. Failures are not yet systematically injected

You can start/stop clusters, but you don’t yet have repeatable failure scenarios like:

kill primary under write load

crash during WAL append

crash during shard migration

partition + heal + verify

Without these, you don’t know how it behaves under stress.

⚠️ 3. No admin/debug surface for validation

Testing distributed systems without introspection is painful.

You need internal-only endpoints like:

GET /admin/members
GET /admin/shards
GET /admin/health


So your tests can assert:

who thinks they are primary

shard states

terms and epochs

Right now you’re mostly blind.

3️⃣ What you should do next (very concrete, no redesign)

This is the exact plan I’d follow if this were my repo.

Step 1 — Add invariant checks in code (highest ROI)

In pkg/shard/manager.go and write paths:

if shard.HasMultiplePrimaries() {
    log.Fatal("INVARIANT VIOLATION: dual primary detected")
}

if !shard.IsPrimary(nodeID) && writeAttempt {
    log.Fatal("INVARIANT VIOLATION: write on non-primary")
}


Fail fast. Loudly. Always.

Silent corruption is worse than crashes.

Step 2 — Add minimal admin endpoints (internal only)

In pkg/server/server.go:

GET /admin/shards
GET /admin/members
GET /admin/state


Return JSON with:

shard_id

primary

followers

term

state

This unlocks real test assertions.

Step 3 — Add process-level failure tests (this is the big one)

Create:

tests/cluster/
  failover.sh
  wal_crash.sh
  partition.sh
  rebalance.sh

Example: failover test
docker compose up -d
sleep 5

# start workload
go run examples/client_example.go &

# kill a node hard
docker kill aegis-node-2

sleep 10

# assert cluster still writable
go run examples/client_example.go --check

docker compose down


This tests:

SIGKILL

elections

real sockets

no split-brain

Step 4 — WAL crash test (mandatory for robustness)
docker compose up -d
sleep 5

docker exec aegis-node-1 aegis set key value

docker kill -s SIGKILL aegis-node-1
docker start aegis-node-1
sleep 5

docker exec aegis-node-2 aegis get key


Assertions:

WAL replay correctness

no stale-term resurrection

Step 5 — Network partition test
docker network disconnect aegis-net aegis-node-3

docker exec aegis-node-1 aegis set k v   # must succeed
docker exec aegis-node-3 aegis set x y   # must fail

docker network connect aegis-net aegis-node-3


This validates:

quorum logic

READ_ONLY enforcement

healing